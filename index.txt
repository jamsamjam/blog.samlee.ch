1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
4:I[8388,["874","static/chunks/874-7618c3fad5f7ba4a.js","974","static/chunks/app/page-d0f3ce9bc5a66fb6.js"],"default"]
7:I[6874,["874","static/chunks/874-7618c3fad5f7ba4a.js","974","static/chunks/app/page-d0f3ce9bc5a66fb6.js"],""]
8:I[9665,[],"OutletBoundary"]
b:I[4911,[],"AsyncMetadataOutlet"]
d:I[9665,[],"ViewportBoundary"]
f:I[9665,[],"MetadataBoundary"]
11:I[6614,[],""]
:HL["/_next/static/media/4cf2300e9c8272f7-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/93f479601ee12b01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/fe8ce8f764dddb50.css","style"]
5:T1ac1,
I’ve been working on a machine learning project involving the comparison of motion patterns extracted from dance videos. Our initial pipeline relied on DTW and several time-based distance metrics, but the results were not as robust as we expected. There was a suggestion to explore the Gromov–Wasserstein (GW) distance, and it turned out to be a very interesting idea! Let's start with..

## Wasserstein Distance
 
Intuitively, it quantifies the minimum "cost" of transforming one probability distribution into another, where the cost is defined in terms of the distance between points in the space. Let's look at the example below.

![Wasserstein Distance](./wasserstein.png)
 from [this lecture](https://www.youtube.com/watch?v=CDiol4LG2Ao).

The cost at i,j is defined as the amount of mass moved ($\pi_{ij}$) times the distance between the points ($d(x_i, y_j)$). The total cost is the sum of all individual costs.

$$
d_{W_1}\!\left( \sum_i \alpha_i \delta_{x_i},\; \sum_j \beta_j \delta_{y_j} \right)
= \min_{\{\pi_{ij}\}} \left\{
    \sum_{i,j} \pi_{ij} \, d(x_i, y_j)
    \;:\;
    \pi_{ij} \ge 0,\;
    \sum_i \pi_{ij} = \beta_j,\;
    \sum_j \pi_{ij} = \alpha_i
\right\}.
$$

For general probability measures with continuous support, the Wasserstein distance is defined as:

![Continuous Wasserstein Distance](./wasserstein-cont.png)

![Wasserstein Distance Formula](./wasserstein-formula.png)
 from [this lecture](https://www.youtube.com/watch?v=gjcz3mQ3asw).

## Gromov-Wasserstein Distance

What if two distributions we want to compare do not live in the same space? For example, we have two graphs with different numbers of nodes, and we want to compare their structures. This is where the Gromov-Wasserstein distance comes into play.

$$
GW_p(X,Y)
\;=\;
\inf_{\pi \in \Pi(\mu_X, \mu_Y)}
\left(
\int_{X \times Y}
\int_{X \times Y}
\big| d_X(x,x') - d_Y(y,y') \big|^p
\;
d\pi(x,y)\, d\pi(x',y')
\right)^{1/p}.
$$

Now we can't use distance between points directly, since they are in different spaces. Instead, we look at the internal structure. 

![Triangulars in Different Spaces](./triangulars.png)

Shape is... the relational structure formed by the distances between points. Even if two objects exist in entirely different coordinate systems, they can still have identical internal structures and thus represent the same shape.

Look at the cost function, we are minimizing the difference in pairwise distances within each space, weighted by the transport plan $\pi$.

## GW Distance in Python

All good! But how to compute coupling π(i,j)? In principle, computing the exact GW coupling requires solving a non convex quadratic optimization problem, which is computationally challenging. In practice, we instead use an entropic regularized version

$$
\min_{\pi} \left(
    \sum_{i,i',j,j'} 
        \pi(i,j)\,\pi(i',j') \,
        \big| d_X(x_i, x_{i'}) - d_Y(y_j, y_{j'}) \big|
    \;+\;
    \varepsilon\, \mathrm{KL}\!\left(\pi \,\middle\|\, \mu_X \otimes \mu_Y \right)
\right)
$$

In Python, we can use the `POT` (Python Optimal Transport) library to compute the GW distance. Here's a simple example:

```python
import numpy as np
import scipy as sp
import ot


def _flatten_poses(poses):
    n_frames = len(poses)
    n_landmarks = poses[0].shape[0]
    # Mediapipe pose landmarks are 3D so each frame is flattened into xyz coordinates.
    return poses[:, :, :3].reshape(n_frames, n_landmarks * 3)


def compute_distance_matrices(poses1, poses2):
    X1 = _flatten_poses(poses1)
    X2 = _flatten_poses(poses2)
    
    D1 = sp.spatial.distance.cdist(X1, X1)
    D2 = sp.spatial.distance.cdist(X2, X2)
    
    D1 /= D1.max()
    D2 /= D2.max()
    
    return D1, D2


def compute_gromov_wasserstein(poses1, poses2, verbose=False, log=False):
    X1 = _flatten_poses(poses1)
    X2 = _flatten_poses(poses2)
    
    D1 = sp.spatial.distance.cdist(X1, X1)
    D2 = sp.spatial.distance.cdist(X2, X2)
    
    D1 /= D1.max()
    D2 /= D2.max()
    
    p = ot.unif(len(X1))
    q = ot.unif(len(X2))
    
    gw_distance, log_dict = ot.gromov.gromov_wasserstein2(
        D1, D2, p, q, 
        loss_fun='square_loss',
        verbose=verbose,
        log=True
    )
    
    transport_plan = ot.gromov.gromov_wasserstein(
        D1, D2, p, q,
        loss_fun='square_loss',
        verbose=verbose
    )
    
    if log:
        return gw_distance, transport_plan, log_dict
    
    return gw_distance, transport_plan


def compute_gw_from_distance_matrices(D1, D2, p=None, q=None, loss_fun='square_loss', verbose=False, log=False):
    if p is None:
        p = ot.unif(D1.shape[0])
    if q is None:
        q = ot.unif(D2.shape[0])
    
    gw_distance, log_dict = ot.gromov.gromov_wasserstein2(
        D1, D2, p, q,
        loss_fun=loss_fun,
        verbose=verbose,
        log=True
    )
    
    transport_plan = ot.gromov.gromov_wasserstein(
        D1, D2, p, q,
        loss_fun=loss_fun,
        verbose=verbose
    )
    
    if log:
        return gw_distance, transport_plan, log_dict
    
    return gw_distance, transport_plan

```

>[!Note]
> More general case with GW distance can be found in the [official doc](https://pythonot.github.io/auto_examples/gromov/plot_gromov.html).

I loaded a pair of dance videos from the same dance (`Video 1`, `Video 2`), another video from a completely different dance (`Video 3`), and extracted pose keypoints using MediaPipe. Each pose is represented as a set of 3D coordinates for various landmarks on the body.

![Distance Matrices Heatmap](./heatmap.png)

Interesting! Distance matrices of Video 1 and Video 2 look quite similar, while that of Video 3 shows more variation. What do you think? The computed GW distances are:

```
Video 1 and Video 2: 0.008441
Video 1 and Video 3: 0.008753
```

In absolute terms the values are close, but if you experiment with other video pairs, you'll notice a clear pattern: GW distances between similar dance styles consistently remain smaller than those between different styles.

![Gromov-Wasserstein Transport Plan](./transport-plan.png)

Well, I find it a bit hard to draw conclusions from the transport plan itself, but looking at the column sums provides some insights.

![Column-sum of Transport Plan](./column-sum.png)

The column sums indicate how much each frame in Video 2 is matched to frames in Video 1. For similar dances, the distribution is more concentrated, suggesting that specific frames correspond closely. In contrast, for different dances, the distribution is more spread out, indicating less direct correspondence.

>[!Important] Summary
>- GW distance provides a flexible way to compare sequences with different lengths or coordinate systems.
>- For motion analysis in particular, it captures structural similarity beyond simple time alignment and serves as a strong complement to DTW based methods.6:T137b,
>[!Caution] Disclaimer
> As I prepare for next semester's physics course, I'll be sharing a series of posts based on *The Feynman Lectures on Physics*. If you're looking for a more in-depth study, I recommend checking out [the original lectures](https://www.feynmanlectures.caltech.edu/III_toc.html). All the pictures (unless otherwise credited) are from the lecture notes.

Quantum entities are neither particles nor clouds.. so how should we define them? Electrons! Their mysterious behavior gives us significant insight into the nature of quantum systems.

### An experiment with bullets

![Bullet Experiment](./bullet.png)

What happens when they all arrive in lumps? We measure probability of arrival — we can think of this as we fire bullets continuously for 10 years and record where each one lands. 
- With only hole 1 open (hole 2 blocked), we observe a distribution $P_1$​.
- With only hole 2 open (hole 1 blocked), we observe $P_2$​.

Here's the interesting part. With both holes open, we get the probability distribution equal to the sum of each hole open alone.
$$
P_{12} = P_1 + P_2
$$
This result is called an observation of *no interference*. 

### An experiment with waves

![Wave Experiment](./wave.png)

Now let's think about waves. The wave source is jiggled up and down, creating ripples on the surface. What we measure isn't just the motion of the water, but more like its energy — something quantified by the *intensity* of the wave.. or the *mean square of the displacement*.
Here, the wave amplitude $h$ is represented as a complex number, and the total intensity $I_{12}$​ is calculated using the complex inner product:

$$
I_{12} = |h_1|^2 + |h_2|^2 + 2|h_1||h_2| \cos \delta
$$

We don't need to dive deep into the theory here. The key question is: why isn't $I_{12}$ simply equal to $I_1 + I_2$?

There's a single wave coming from the source, spreading across the surface. When it reaches the barrier with two slits, each slit acts like a new source of ripples. The waves emerging from each slit spread outward and overlap, which means they _interfere_ with each other.

### An experiment with electrons

![Electron Experiment](./electron.png)

It’s really difficult to set up this kind of experiment in a real-world setting — the apparatus would have to be built on an incredibly small scale. But thanks to many experiments that have been conducted over the years, we know how electrons behave in such scenarios.

What’s really fascinating is that electrons are **particles**, yet they behave like **waves**.  
Unlike water waves, they can’t be split or spread out, so they always arrive in lumps. So why do we still see interference?
If you look at (c), we observe that when we close slit 1 (or 2) $P_{12}$ is sometimes higher than the sum, or sometimes less. In other words, opening or closing a slit increase or decreases the intensity. So we say electrons are *sometimes like a particle and sometimes like a wave*.

Let's now change the experiment slightly. Suppose we shine a light near the slits behind the wall at (a), to *watch* the electrons. We might see a flash at hole 1 or hole 2 — and then we know which slit the electron went through. And we record the distributions $P_1$ and $P_2$ accordingly. How do we get $P_{12}$? This time we can count the time we hear the click sound from the detector. What do we have now?

$$
P'_{12} = P_1 + P_2
$$

Wait — what just happened? When we look at the electrons, their distribution changes. That is, observation destroys interference.


>[!Note]
>We've seen how electrons, despite being particles, show interference patterns. But why do electrons create the patterns? They don't just take one path. Instead, they *explore all possible paths*, and each path contributes a probability amplitude. These amplitudes interfere — sometimes constructively, sometimes destructively — which leads to the observed pattern.
>
>But this idea of sum over paths isn't limited to electrons. It applies to photons (particles of light) as well. This is a core idea of quantum theory, often described as Feynman’s *sum over histories*. A great video that gives you an intuition about this (and quantum physics) can be found [here](https://www.youtube.com/watch?v=KTzGBJPuJwM).


## Summary

<img src="./summary.png" alt="Summary" width="65%">

Quantum mechanics is different from classical mechanics in a way that we can only predict the odds — we do not know exactly what would happen.

Heisenberg stated the uncertainty principle.

$$
\Delta x \cdot \Delta p \geq \frac{\hbar}{2}
$$

It can be interpreted as *the uncertainties in the position and momentum of a particle at any instant must have their product greater than or equal to half the reduced Planck constant*. we can't determine the momentum and the position of a particle sufficiently accurately. For a more detailed explanation of the Heisenberg Uncertainty Principle, see [here](https://www.youtube.com/watch?v=MBnnXbOM5S4).
0:{"P":null,"b":"kXhudBOHdKfyWPEYh0QLl","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/fe8ce8f764dddb50.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__variable_188709 __variable_9a8899 antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-4xl mx-auto p-8","children":[["$","header",null,{"className":"mb-10","children":[["$","h1",null,{"className":"text-3xl font-bold mt-4 mb-1","children":"Sam's Blog"}],["$","p",null,{"className":"text-base text-gray-600 dark:text-gray-400","children":"On dev, tech, and whatever else I find interesting"}]]}],["$","$L4",null,{"posts":[{"slug":"gw-dist","title":"Gromov-Wasserstein Distance","date":"2025-12-04T14:58:07.000Z","description":"An overview of the Gromov-Wasserstein distance and its applications in my ML project","tags":["ML"],"content":"$5"},{"slug":"quantum-1","title":"Quantum #1: How to Understand Quantum","date":"2025-07-11T16:04:17.000Z","description":"An introduction to the fundamental concepts of quantum mechanics","tags":["Quantum"],"content":"$6"},{"slug":"app-icon","title":"Expo Build: App Icon Not Showing Up?","date":"2025-05-04T17:59:53.000Z","description":"How I fixed the missing app icon issue in Expo builds","tags":["App"],"content":"\nWhile building my app with Expo, I ran into a frustrating issue: the app icon wasn’t showing up in the final build. I had the correct `icon` path in `app.json`, but no matter what I tried it didn’t work, until I ran..\n\n\n```bash\nnpx expo prebuild --clean\nnpx expo prebuild\neas build --platform android --clear-cache\neas build --platform ios --clear-cache\n```\n\n![App Store Connect icons](./icon.png)\n\n**Note:** The icon still didn’t show up correctly in Apple’s Transporter. It kept displaying the default gray grid icon. However, after I uploaded it on Apple Connecct it the icon appeared as expected.\n\n"}]}],["$","footer",null,{"className":"mt-3 pt-8 text-center","children":["$","p",null,{"className":"text-sm text-gray-500 dark:text-gray-400","children":["© 2025 ",["$","$L7",null,{"href":"https://samlee.ch","children":["$","span",null,{"className":"underline hover:text-gray-700 dark:hover:text-gray-300","children":"Sam Lee"}]}],". All rights reserved."]}]}]]}],null,["$","$L8",null,{"children":["$L9","$La",["$","$Lb",null,{"promise":"$@c"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","BXrBdicLqTnS8yNzHEYLVv",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$Lf",null,{"children":"$L10"}]]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
10:["$","div",null,{"hidden":true,"children":["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]}]
a:null
e:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
9:null
c:{"metadata":[["$","title","0",{"children":"Sam's Blog"}],["$","meta","1",{"name":"description","content":"On dev, tech, and whatever else I find interesting"}],["$","link","2",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}]],"error":null,"digest":"$undefined"}
14:{"metadata":"$c:metadata","error":null,"digest":"$undefined"}
